%! TEX root = ../thesis.tex
\section{Inference Algorithm}
\label{sec:inference}

% Introduction to section.
In this section, we derive an efficient inference algorithm for our model.
For brevity, we focus on explaining the main ideas behind the algorithm.
A reference software implementation, available online at \url{https://github.com/lucasmaystre/kickscore}, complements the description provided here.

% Notation & preliminaries.
We begin by introducing some notation.
Let $\mathcal{D} = \{ (\bm{x}_n, t_n, y_n) : n \in [N] \}$ be a dataset of $N$ independent observations, where for conciseness we fold the two opponents $\bm{x}_{n,i}$ and $\bm{x}_{n,j}$ into $\bm{x}_n \doteq \bm{x}_{n,i} - \bm{x}_{n,j}$, for each observation\footnote{%
This enables us to write the score difference more compactly.
Given an observation at time $t^*$ and letting $\bm{x} \doteq \bm{x}_i - \bm{x}_j$, we have $s_i - s_j = \bm{x}_i\Tr \bm{s}(t^*) - \bm{x}_j\Tr \bm{s}(t^*) = \bm{x}\Tr \bm{s}(t^*)$.
}.
Let $\mathcal{D}_m \subseteq [N]$ be the subset of observations involving feature $m$, i.e., those observations for which $x_{nm} \ne 0$, and let $N_m = \lvert \mathcal{D}_m \rvert$.
Finally, denote by $\bm{s}_m \in \mathbf{R}^{N_m}$ the samples of the latent score process at times corresponding to the observations in $\mathcal{D}_m$.
The joint prior distribution of these samples is $p(\bm{s}_m) = \mathcal{N}(\bm{0}, \bm{K}_m)$, where $\bm{K}_m$ is formed by evaluating the covariance function $k_m(t, t')$ at the relevant times.

% Bayesian inference.
We take a Bayesian approach and seek to compute the posterior distribution
\begin{align}
\label{eq:truepost}
p(\bm{s}_1, \ldots, \bm{s}_M \mid \mathcal{D}) \propto \prod_{m = 1}^M p(\bm{s}_m) \prod_{n = 1}^N p[y_n \mid \bm{x}_n\Tr \bm{s}(t_n)].
\end{align}
As the scores are coupled through the observations, the posterior no longer factorizes over $\{ \bm{s}_m \}$.
Furthermore, computing the posterior is intractable if the likelihood is non-Gaussian.

% Approximate inference.
To overcome these challenges, we consider a mean-field variational approximation \citep{wainwright2008graphical}.
In particular, we assume that the posterior can be well-approximated by a multivariate Gaussian distribution that factorizes over the features:
\begin{align}
\label{eq:approxpost}
p(\bm{s}_1, \ldots, \bm{s}_M \mid \mathcal{D})
    \approx q(\bm{s}_1, \ldots, \bm{s}_M)
    \doteq \prod_{m = 1}^M \mathcal{N}(\bm{s}_m \mid \bm{\mu}_m, \bm{\Sigma}_m).
\end{align}
Computing this approximate posterior amounts to finding the variational parameters $\{\bm{\mu}_m, \bm{\Sigma}_m \}$ that best approximate the true posterior.
More formally, the inference problem reduces to the optimization problem
\begin{align}
\label{eq:varopt}
\min_{\{\bm{\mu}_m, \bm{\Sigma}_m \}} \mathrm{div} \left[ p(\bm{s}_1, \ldots, \bm{s}_M \mid \mathcal{D}) \;\Vert\; q(\bm{s}_1, \ldots, \bm{s}_M) \right],
\end{align}
for some divergence measure $\mathrm{div}(p \Vert q) \ge 0$.
We will consider two different such measures in Section~\ref{sec:inf-pseudo-obs}.

% Gaussian pseudo-observation viewpoint.
A different viewpoint on the approximate posterior is as follows.
For both of the variational objectives that we consider, it is possible to rewrite the optimal distribution $q(\bm{s}_m)$ as
\begin{align*}
q(\bm{s}_m) \propto p(\bm{s}_m) \prod_{n \in \mathcal{D}_m} \mathcal{N}[s_{mn} \mid \tilde{\mu}_{mn}, \tilde{\sigma}^2_{mn}].
\end{align*}
Letting $\mathcal{X}_n \subseteq [M]$ be the subset of features such that $x_{nm} \ne 0$, we can now reinterpret the variational approximation as transforming every observation $(\bm{x}_n, t_n, y_n)$ into several independent \emph{pseudo-observations} with Gaussian likelihood, one for each feature $m \in \mathcal{X}_n$.
Instead of optimizing directly $\{ \bm{\mu}_m, \bm{\Sigma}_m \}$ in~\eqref{eq:varopt}, we can alternatively choose to optimize the parameters $\{ \tilde{\mu}_{mn}, \tilde{\sigma}^2_{mn} \}$.
For any feature $m$, given the pseudo-observations' parameters $\tilde{\bm{\mu}}_m$ and $\tilde{\bm{\sigma}}_m^2$, computing $q(\bm{s}_m)$ becomes tractable (c.f. Section~\ref{sec:inf-posterior}).

% Algorithm
An outline of our iterative inference procedure is given in Algorithm~\ref{alg:inference}.
Every iteration consists of two steps:
\begin{enumerate}
\item updating the pseudo-observations' parameters given the true observations and the current approximate posterior (lines \ref{li:begin-param}--\ref{li:end-param}), and
\item recomputing the approximate posterior given the current pseudo-observation (lines \ref{li:begin-post} and \ref{li:end-post}).
\end{enumerate}
Convergence is declared when the difference between two successive iterates of $\{ \tilde{\mu}_{mn} \}$ and $\{ \tilde{\sigma}_{mn}^2 \}$ falls below a threshold.
Note that, as a by-product of the computations performed by the algorithm, we can also estimate the log-marginal likelihood of the data, $\log p(\mathcal{D})$.

\algtext*{EndFor}
\begin{algorithm}[t]
  \caption{Model inference.}
  \label{alg:inference}
  \begin{algorithmic}[1]
    \Require $\mathcal{D} = \{ (\bm{x}_n, t_n, y_n) : n \in [N] \}$
    \State $\tilde{\bm{\mu}}_m, \tilde{\bm{\sigma}}^2_m \gets \bm{0}, \bm{\infty} \quad \forall m$
    \State $q(\bm{s}_m) \gets p(\bm{s}_m) \quad \forall m$
    \Repeat
      \For{$n = 1, \ldots, N$}  \label{li:begin-param}
        \State $\bm{\delta} \gets \textsc{Derivatives}(\bm{x}_n, y_n)$ \label{li:derivatives}
        \For{$m \in \mathcal{X}_n$}
          \State $\tilde{\mu}_{mn}, \tilde{\sigma}^2_{mn} \gets \textsc{UpdateParams}(x_{nm}, \bm{\delta})$ \label{li:updateparams}
        \EndFor
      \EndFor \label{li:end-param}
      \For{$m = 1, \ldots, M$} \label{li:begin-post}
        \State $q(\bm{s}_m) \gets \textsc{UpdatePosterior}(\tilde{\bm{\mu}}_m, \tilde{\bm{\sigma}}^2_m)$ \label{li:updateposterior}
      \EndFor \label{li:end-post}
    \Until convergence
  \end{algorithmic}
\end{algorithm}

\paragraph{Running Time}
In Appendix~\ref{app:inference}, we show that \textsc{Derivatives} and \textsc{UpdateParams} run in constant time.
In Section~\ref{sec:inf-posterior}, we show that \textsc{UpdatePosterior} runs in time $O(N_m)$.
Therefore, if we assume that the vectors $\{ \bm{x}_n \}$ are sparse, the total running time per iteration of Algorithm~\ref{alg:inference} is $O(N)$.
Furthermore, each of the two outer \emph{for} loops (lines \ref{li:begin-param} and \ref{li:begin-post}) can be parallelized easily, leading in most cases to a linear acceleration with the number of available processors.


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Updating the Pseudo-Observations}
\label{sec:inf-pseudo-obs}

% Intro to the subsection.
The exact computations performed during the first step of the inference algorithm---updating the pseudo-observations---depend on the specific variational method used.
We consider two: expectation propagation \citep{minka2001family}, and reverse-KL variational inference~\citep{blei2017variational}.
The ability of Algorithm~\ref{alg:inference} to seamlessly adapt to either of the two methods is valuable, as it enables practitioners to use the most advantageous method for a given likelihood function.
Detailed formulae for \textsc{Derivatives} and \textsc{UpdateParams} can be found in Appendix~\ref{app:inference}.

\subsubsection{Expectation Propagation}

% Cavity and hybrid distributions.
We begin by defining two distributions.
The \emph{cavity} distribution $q_{-n}$ is the approximate posterior without the pseudo-observations associated with the $n$th datum, that is,
\begin{align*}
q_{-n}(\bm{s}_1, \ldots, \bm{s}_M) \propto \frac{q(\bm{s}_1, \ldots, \bm{s}_M)}{
        \prod_{m \in \mathcal{X}_n} \mathcal{N}[s_{mn} \mid \tilde{\mu}_{mn}, \tilde{\sigma}^2_{mn}]}.
\end{align*}
The \emph{hybrid} distribution $\hat{q}_n$ is given by the cavity distribution multiplied by the $n$th likelihood factor, i.e.,
\begin{align*}
\hat{q}_n(\bm{s}_1, \ldots, \bm{s}_M) \propto q_{-n}(\bm{s}_1, \ldots, \bm{s}_M) p[y_n \mid \bm{x}_n\Tr \bm{s}(t_n)].
\end{align*}
Informally, the hybrid distribution $\hat{q}_n$ is ``closer'' to the true distribution than $q$.

% Update rule.
Expectation propagation (EP) works as follows. At each iteration and for each $n$, we update the parameters $\{ \tilde{\mu}_{mn}, \tilde{\sigma}_{mn} : m \in \mathcal{X}_n \}$ such that $\KL( \hat{q}_n \Vert q )$ is minimized.
To this end, the function \textsc{Derivatives} (on line~\ref{li:derivatives} of Algorithm~\ref{alg:inference}) computes the first and second derivatives of the log-partition function
\begin{align}
\label{eq:logpart}
\log \mathbf{E}_{q_{-n}} \left\{ p[y_n \mid \bm{x}_n\Tr \bm{s}(t_n)] \right\}
\end{align}
with respect to ${\mu}_{-n} \doteq \mathbf{E}_{q_{-n}}[\bm{x}_n\Tr \bm{s}(t_n)]$.
These computations can be done in closed form for the widely-used probit likelihood, and they involve one-dimensional numerical integration for most other likelihoods.
EP has been reported to result in more accurate posterior approximations on certain classification tasks \citep{nickisch2008approximations}.

\subsubsection{Reverse KL Divergence}

% Variational objective.
This method (often referred to simply as \emph{variational inference} in the literature) seeks to minimize $\KL(q \Vert p)$, i.e., the KL divergence from the approximate posterior $q$ to the true posterior $p$.

% Update rule.
To optimize this objective, we adopt the approach of \citet{khan2017conjugate}.
In this case, the function \textsc{Derivatives} computes the first and second derivatives of the expected log-likelihood
\begin{align}
\label{eq:exp-ll}
\mathbf{E}_q \left\{ \log p[y_n \mid \bm{x}_n\Tr \bm{s}(t_n)] \right\}
\end{align}
with respect to $\mu \doteq \mathbf{E}_q[\bm{x}_n\Tr \bm{s}(t_n)]$.
These computations involve numerically solving two one-dimensional integrals.

% Comparison to EP.
In comparison to EP, this method has two advantages.
The first is theoretical:
If the likelihood $p(y \mid d)$ is log-concave in $d$, then the variational objective has a unique global minimum, and we can guarantee that Algorithm~\ref{alg:inference} converges to this minimum \citep{khan2017conjugate}.
The second is numerical:
Excepted for the probit likelihood, computing~\eqref{eq:exp-ll} is numerically more stable than computing~\eqref{eq:logpart}.


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Updating the Approximate Posterior}
\label{sec:inf-posterior}

% Problem formulation.
The second step of Algorithm~\ref{alg:inference} (lines~\ref{li:begin-post} and~\ref{li:end-post}) solves the following problem, for every feature $m$.
Given Gaussian pseudo-observations $\{ \tilde{\mu}_{mn}, \tilde{\sigma}_{mn} : n \in \mathcal{D}_m \}$ and a Gaussian prior $p(\bm{s}_m) = \mathcal{N}(\bm{0}, \bm{K}_m)$, compute the posterior
\begin{align*}
q(\bm{s}_m) \propto p(\bm{s}_m) \prod_{n \in \mathcal{D}_m} \mathcal{N}[s_{mn} \mid \tilde{\mu}_{mn}, \tilde{\sigma}^2_{mn}].
\end{align*}
This computation can be done independently and in parallel for each feature $m \in [M]$.

% Naive approach using matrix inversion.
A naive approach is to use the self-conjugacy properties of the Gaussian distribution directly.
Collecting the parameters of the pseudo-observations into a vector $\tilde{\bm{\mu}}_m$ and a diagonal matrix $\tilde{\bm{\Sigma}}_m$, the parameters of the posterior $q(\bm{s}_m)$ are given by
\begin{align}
\label{eq:batch}
\bm{\Sigma}_m = (\bm{K}_m^{-1} + \tilde{\bm{\Sigma}}_m^{-1})^{-1}, \qquad
\bm{\mu}_m    = \bm{\Sigma}_m \tilde{\bm{\Sigma}}_m^{-1} \tilde{\bm{\mu}}_m.
\end{align}
Unfortunately, this computation runs in time $O(N_m^3)$, a cost that becomes prohibitive if some features appear in many observations.

% State space formulation.
Instead, we use an alternative approach that exploits a link between temporal Gaussian processes and state-space models \citep{hartikainen2010kalman, reece2010introduction}.
Without loss of generality, we now assume that the $N$ observations are ordered chronologically, and, for conciseness, we drop the feature's index and consider a single process $s(t)$.
The key idea is to augment $s(t)$ into a $K$-dimensional vector-valued Gauss-Markov process $\bar{\bm{s}}(t)$, such that
\begin{align*}
\bar{\bm{s}}(t_{n+1}) = \bm{A}_n \bar{\bm{s}}(t_n) + \bm{\varepsilon}_n,
    \qquad \bm{\varepsilon}_n \sim \mathcal{N}(\bm{0}, \bm{Q}_n)
\end{align*}
where $K \in \mathbf{N}_{>0}$ and $\bm{A}_n, \bm{Q}_n \in \mathbf{R}^{K \times K}$ depend on the time interval $\Abs{t_{n+1} - t_n}$ and on the covariance function $k(t, t')$ of the original process $s(t)$.
The original (scalar-valued) and the augmented (vector-valued) processes are related through the equation
\begin{align*}
s(t) = \bm{h}\Tr \bar{\bm{s}}(t),
\end{align*}
where $\bm{h} \in \mathbf{R}^K$ is called the \emph{measurement vector}.

% Corresponding graphical model and inference.
Figure~\ref{fig:ssm} illustrates our model from a state-space viewpoint.
It is important to note that the mutual time dependencies of Figure~\ref{fig:model} have been replaced by Markovian dependencies.
In this state-space formulation, posterior inference can be done in time $O(K^3 N)$ by using the Rauch--Tung--Striebel smoother \citep{sarkka2013bayesian}.

\begin{figure}[t]
  \centering
  \input{figures/pgm-ssm}
  \caption{State-space reformulation of our model.
  With respect to the representation in Figure~\ref{fig:model}, the number of latent variables has increased, but they now form a Markov chain.}
  \label{fig:ssm}
\end{figure}

\paragraph{From Covariance Functions to State-Space Models}
A method for converting a process $s(t) \sim \GP[\bm{0}, k(t, t')]$ into an equivalent Gauss-Markov process $\bar{\bm{s}}(t)$ by explicit construction of $\bm{h}$, $\{\bm{A}_n\}$ and $\{\bm{Q}_n\}$ is given in \citet{solin2016stochastic}.
All the covariance functions described in Section~\ref{sec:covariances} lead to exact state-space reformulations of order $K \leq 3$.
The composition of covariance functions through addition or multiplication can also be treated exactly and automatically.
Some other covariance functions, such as the squared-exponential function or periodic functions \citep{rasmussen2006gaussian}, cannot be transformed exactly but can be approximated effectively and to arbitrary accuracy \citep{hartikainen2010kalman, solin2014explicit}.

% Covariance function viewpoint is more intuitive.
Finally, we stress that the state-space viewpoint is useful because it leads to a faster inference procedure; but defining the time dynamics of the score processes in terms of covariance functions is much more intuitive.


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Predicting at a New Time}

Given the approximate posterior $q(\bm{s}_1, \ldots, \bm{s}_M)$, the probability of observing outcome $y$ at a new time $t^*$ given the feature vector $\bm{x}$ is given by
\begin{align*}
p(y \mid \bm{x}, t^*) = \int_\mathbf{R} p(y \mid z) p(z) dz,
\end{align*}
where $z = \bm{x}\Tr \bm{s}(t^*)$ and the distribution of $s_m(t^*)$ is derived from the posterior $q(\bm{s}_m)$.
By using the state-space formulation of the model, the prediction can be done in constant time \citep{saatci2012scalable}.
