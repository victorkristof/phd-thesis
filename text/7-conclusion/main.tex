%! TEX root = ../thesis.tex
\chapter{Conclusion}
\label{ch:conclusion}

In this thesis, we have answered specific questions about social processes from a machine-learning and data-mining viewpoint.
In order to propose predictive models that enable interpretation through the learned parameters, we have built upon the literature on discrete-choice models, which we have combined with latent-factor models, Bayesian statistics, and generalized linear models.
We have demonstrated that discrete-choice analysis offers a principled and powerful approach to modeling social processes, as making choices is inherent in human behaviour.

In Chapter~\ref{ch:peerproduction}, we have studied the social dynamics behind group collaborations for the collective creation of content, such as in Wikipedia and the Linux kernel.
We have proposed a new discrete-choice model inspired from the Bradley-Terry and Rasch models, which incorporates ideas from collaborative filtering.
The model enables us to identify controversial Wikipedia articles and core Linux components that are crucial to the system.
We have improved the predictive performance by including latent factors, which in turn have helped us understand how users edit some Wikipedia articles:
They are either ``experts'' in popular culture or in high culture, but not both.

In Chapter~\ref{ch:lawmaking}, we have studied, through the lens of peer-production systems, the law-making process in the European Union.
To capture the conflictive structure inherent in this process, we have designed a model inspired from the multinomial logit and Rasch models, which we have enhanced with natural language processing techniques.
We have quantified the controversy of laws and proposed intuitive visualizations by representing each law as the conflict graph of its edits.
We have also identified features of the edits that correlate with a higher probability of acceptance:
For example, inserting short edits, providing a justification for the change, deleting ``human rights'', and having the parliamentarian in charge of the law sponsor the edit are all factors that contribute to acceptance.

In Chapter~\ref{ch:predikon}, we have developed an algorithm that combines matrix factorization and generalized linear models for predicting the popular vote of elections and referenda from partial, regional results.
This algorithm learns representations of votes and regions to capture ideological and cultural voting patterns (e.g., rural/urban, liberal/conservative, etc.).
Its predictions are also accurate:
In Switzerland, for example, it is able to predict referendum votes with an accuracy of 99\% and a mean absolute error of less than 1\% using only 5\% of the results observed in municipalities.
We have deployed our algorithm on a Web platform to make real-time predictions for referenda in Switzerland.

In Chapter~\ref{ch:climpact}, we have studied how people perceive the carbon footprint of their day-to-day actions.
We have cast this problem as a comparison problem between pairs of actions (\textit{e.g.}, between intercontinental flights and using household appliances) and developed a statistical model of relative comparisons reminiscent of the Thurstone model in psychometrics.
The model learns users' perception as the parameters of a Bayesian linear regression, which enables us to derive an active-learning algorithm to select the optimal pairs of actions to probe.
Because no suitable data existed for answering these questions, we built a Web interface to collect comparison data from students on our university campus.
Our results show that users tend to overestimate actions with low carbon footprint and underestimate actions with high carbon footprint.

Finally, in Chapter~\ref{ch:kickscore}, we have developed a dynamic choice-model that enables the parameters to vary over time.
We achieve this by replacing the static parameters by continuous-time Gaussian processes.
We have also developed an efficient inference algorithm that computes an approximate Bayesian posterior distribution in a few linear-time iterations over the data.
We have shown experimentally on several datasets of (e-)sports that this model outperforms competing approaches in terms of predictive performance, scales to millions of observations, and generates compelling visualizations of the parameters' dynamics.
We have deployed our approach as a real-world application on the Web for predicting football matches in European leagues and international competitions.

\paragraph{Ethical Considerations}
Studying human behavior and addressing social problems from a computational viewpoint induces a risk of abuses.
This is especially true for political processes, as in the infamous scandal of Cambridge Analytica in 2016.
After submitting our paper~\citep{kristof2021war} forming the basis of Chapter~\ref{ch:lawmaking} to the Web Conference 2021, one anonymous reviewer expressed concerns regarding the use of machine learning for making decisions in law making, and whether our findings in Section~\ref{lmp:sec:results} could help adversarial attacks.
We answer to such concerns by precising that we do not propose to rely on our models for making decisions, such as whether an edit should be accepted or not.
Our goal is to understand the factors correlated with the acceptance of edits, and thereby gain insights into the law-making processes.
These correlations do not imply a causal relationship that would benefit potential adversarial attackers.
Nevertheless, even if such a relationship were to exist, we prefer that these findings are published in an open research community, where possible countermeasures to such attacks could be thought of for the public good, rather than them being discovered by a company or an influence group that might use them in an opaque manner to push private interests.

Our vote prediction algorithm of Chapter~\ref{ch:predikon} also triggered private concerns regarding its potential effect on the final outcome.
If voters see early predictions of the outcome of an election or referendum, will they decide not to vote at all (if the prediction is in their favor) or, on the contrary, encourage more people to do so (if the prediction goes against their preference) in order to swing the result?
In the political science literature of election forecasting, these are called the \emph{underdog} and the \emph{bandwagon} effects, and it is unclear which one prevails.
However, a recent paper that studies the effect of probabilistic forecasting in the 2016 U.S.\ election concludes that ``forecasting can fundamentally alter the information environment available to potential voters, with the potential to change the outcome of elections''~\citep{westwood2020projecting}.
For that reason, some countries, such as France, Spain, Italy, and Canada, enforce \emph{election silence}:
Polling and political campaigning (including predictions) are forbidden some time prior to the voting day to prevent influencing voting behavior and election outcome.
In Switzerland, voting offices close at 12:00pm on the Sunday of the vote.
Our predictions are made during ballot counting, which takes a few hours and starts only when voting has closed everywhere.
Hence, our predictions cannot influence the voting behavior, because it is impossible to vote when our predictions are made public.

\paragraph{Perspective on Interpretability and Causality}

The results of this thesis rely on correlations between features and predicted outputs.
For example, in Chapter~\ref{ch:lawmaking}, we uncover features of law edits that correlate well with higher probability of acceptance.
While these findings enable us to shed light on the problems we address, they could be made stronger by taking a causal inference perspective.
In particular, a rigorous evaluation of potential confounding factors would reinforce our statistical models and conclusions.
For example, in Chapter~\ref{ch:lawmaking}, controversy might be a confounding factor:
Many edits might be rejected because the laws are controversial, but controversy is also modeled as a parameter of the law proposals.
Causal inference and reasoning is rapidly expanding in the machine-learning community.
Discovering causal relationships could be used to derive new insights from the present work and to develop our methods further.

\paragraph{Perspective on Methodological Uncertainties}

% Parameter uncertainty.
By definition, computational models are approximate representations of (complex) realities.
Human behaviour, in particular, is uncertain by essence, and models of social processes are only as good as the datasets they rely on.
In the presence of noisy data, it becomes crucial to quantify uncertainty and propagate it through parameter estimation.
This enables a model to provide predictive distributions rather than point-wise predictions.
Bayesian inference offers a principled approach to achieve uncertainty quantification and propagation.
In this work, we have relied on such methods only in Chapter~\ref{ch:kickscore} and, to some extent, in Chapter~\ref{ch:climpact}.
We believe that the other chapters, and the \textsc{SubSVD-GLM} algorithm of Chapter~\ref{ch:predikon} in particular, would benefit from Bayesian inference to provide more informative and robust predictions.

% Structural uncertainty.
The discrete-choice models on which the present work is based, such as the Bradley-Terry, the Thurstone, and the multinomial logit models, are subject to structural uncertainty because they assume that the alternatives in the choice set are independent.
Clearly, this strong assumption might limit the depth of some analyses.
Resorting to models that encode dependencies between alternatives offers a natural and promising direction to further exploit the structure of the problems at hand.
For example, the mixed logit, nested logit, and multinomial probit models all enable this by encoding correlations through the joint distribution of the noise model (see Section~\ref{in:sec:models}).
To the best of our knowledge, combining matrix factorization techniques with these models has not yet been explored and opens up fascinating research directions that could lead to new methodologies in discrete-choice analysis and preference learning.

\paragraph{Perspective on Socio-Environmental Processes}

This thesis is written at a time when the global political spectrum is more polarized than ever and in a society that faces the grand environmental challenges of climate change and biodiversity loss.
Although currently being mostly part of the problem, computer science and machine-learning algorithms can become part of the solution.
Impactful policies require ambitious target setting and effective implementation.
They require combining (1) top-down processes, \textit{i.e.}, how policy-makers shape laws, and (2) bottom-up processes, \textit{i.e.}, how individuals make choices in their daily life.
For (1), machine-learning methods can help to discover influence networks and lobbying activities in political processes.
For (2), understanding how people make choices and change their opinions over time gives a starting point to bridge the gap between policy and implementation.
Processing large-scale datasets of legal texts, parliamentary speeches, and social media activities with recent methods in language modeling, latent-factor models, and network science offers a promising direction to study the hidden influence processes and to understand people's behaviour in law-making and political participation.
Monitoring societal currents and making the results available to the general public can increase transparency into political processes and help shape fair, ethical, and effective policies.

\paragraph{A Call for Interdisciplinary Research}

Tackling social-science problems from a machine-learning perspective gave us the advantage of agnostic analyses, but sometimes at the expense of some limitation in the analysis.
Indeed, collaborating with political scientists, economists, psychologists, sociologists, climate scientists, and ecologists would provide expert knowledge to strengthen our findings.
As computer science increasingly percolates through other scientific domains, computational methods are often used only as a means to an end, rather than as a source of innovative approaches and fresh viewpoints.
Interdisciplinary research can act as an effective catalyst for scientific progress, as major breakthroughs often take place by crossing ideas from different fields.
For example, the sequencing of the human genome in the 1990s required the collective efforts of physicists, chemists, biologists, and computer scientists.
Today, at a time when conspiracy theories disseminate doubt and threaten the acceptance of facts, interdisciplinary research could restore trust in science by reinforcing the credibility of scientific results and enhancing their scope.
Only synergistic collaborations with other fields will enable computer science to unleash its true potential for transformative societal good.
