%! TEX root = ../thesis.tex
\section{Experimental Results}
\label{lmp:sec:results}

\subsection{Baselines}
% - Baseline:
%   - Random
%   - Advanced random -> adapts to the size of conflict
%   - WoW(.) and WoW(R)
% - Introduce the WoW model (starting from the previous paper); this model will serve as a baseline.
%   - We adopt the terminology of us et al.

We start by introducing the baselines against which we compare our models.
For each baseline and for our models, we assume a set of~$K$ conflicting edits \mbox{$\mathcal{C} = \{ a, b, \ldots \}$} proposed on dossier~$i$, for which we want to model the probability that an edit~$a \in \mathcal{C}$ is accepted over edits~$b, \ldots$ on this dossier.
We denote this probability by~$\Prob{ a \succ_i \mathcal{C} - \{ a\} }$, and we denote the probability that the status quo wins, \textit{i.e.}, that the original text proposed by the Commission is kept, by~$\Prob{ i \succ \mathcal{C} } = 1 - \sum_{a\in \mathcal{C}} \Prob{ a \succ_i \mathcal{C} - \{a\} }$.

\paragraph{Naive Classifier}

The \textit{naive classifier} predicts a uniform probability for each outcome, \textit{i.e.}, for each of the conflicting edits or the status quo to win, as
\begin{equation*}
	\Prob{ a \succ_i \mathcal{C} - \{ a\} } = \Prob{ i \succ \mathcal{C} } = \frac{1}{K + 1}.
\end{equation*}

\paragraph{Random Classifier}

The \textit{random classifier} learns the prior probability~$p^{(K)}$ that the status quo wins for each conflict size~$\vert \mathcal{C} \vert = K$, and it predicts
\begin{equation*}
	\Prob{ i \succ \mathcal{C} } = p^{(K)}.
\end{equation*}
It predicts uniformly each of the edits to win as
\begin{equation*}
	\Prob{ a \succ_i \mathcal{C} - \{ a\} } = \frac{1-p^{(K)}}{K}.
\end{equation*}

\subsection{Experimental Setting}

% Explain the metrics we use for evaluation.
We report the cross-entropy loss to evaluate the baselines and our models.
Let~$( \mathcal{C}_n, i_n, l_n )$ be an observation.
We compute
\begin{equation}
	\ell_n = \begin{cases}
		-\log p(l_n \succ_{i_n} \mathcal{C}_n - \{l_n\} ) & \text{if $l_n \in \mathcal{C}_n$}, \\
		-\log p(i_n \succ \mathcal{C}_n)                  & \text{if $l_n = i_n$}.
	\end{cases}
\end{equation}
We report the average value for all~$N$ points in our test set as~$\ell = \frac{1}{N} \sum_n \ell_n$.
%, and we compute the 99\% confidence intervals around this average.
% - Explain the training and testing procedure
% - Explain how we chose the best hyperparameters
We randomize our dataset and we split it into 80\% for training, 10\% for validation, and 10\% for the final evaluation.
Note that an edit can be involved in several conflicts.
For example, in Figure~\ref{lmp:fig:amendment}, edit~$c$ is in involved in two conflicts: $\mathcal{C}_1 = \{ c, d \}$ and $\mathcal{C}_2 = \{ c, e \}$.
Hence, we assign conflicts to each set so that an edit is present in exactly one set.
We combine both the training and the validation sets to fit our model before evaluating it on the test set.
We set the number of latent dimensions~$L$ and the regularizers, and we choose the best word embeddings, by held-out validation.
% Please refer to Appendix~\ref{app:hyperparameters} for a complete list of the best hyperparameters that we used.
This results in fastText of dimension~$D = D' = 10$, with bigrams.

\subsection{Predictive Performance}
% - Give and discuss the predictive performance of all models
%   - Predicting a new edit
% - Plot the results in terms of
%   - Log-loss
%   - Accuracy?
%   - F1-Score/AUC?

We show in Figure~\ref{lmp:fig:results} the overall performance of all variations of our model (with and without explicit, latent, and text features) over EP7 and EP8, and we compare them against the naive and the random predictors, as well as against the \wow{}\ model.
All our models outperform the baselines, and \wow{XLT} outperforms all other models.
Including explicit features improves the performance of the predictions in terms of the cross entropy by 7\% for EP7 and 6\% for EP8 over the simpler \wow{}\ model.
On EP7, \wow{L}\ improves the performance by 12\% and \wow{T} by 7\%, whereas for EP8 the difference between the two models is smaller (10\% increase for \wow{L}\ and 8\% for \wow{T}).
Hence, the text features provide a greater improvement for EP8 than for EP7, while the latent features provide a greater improvement for EP7 than for EP8.
The difference between \wow{XL}\ and \wow{L}\ (0.010 for EP7 and 0.013 for EP8) is less than the difference between \wow{XT}\ and \wow{T}\ (0.034 for EP7 and 0.035 for EP8), as the latent features absorb the effects of the explicit features more than the text features do.
Finally, combining the text and latent features provides high performance, but further combining them with explicit features leads to the best performance.

\begin{figure}
	\centering
	\includegraphics{lmp-results}
	\caption{%
		Average cross-entropy loss of the baselines and our models.
		Combining the explicit, latent, and text features help obtain the best performance.
	}
	\label{lmp:fig:results}
\end{figure}

\subsection{Error Analysis by Conflict Size}
% - Compute the performance of high- and low-controversy dossiers
%   - Dynamics of law-making is different depending of the level of controversy
%   - We can not use the same predictor, but the hybrid models bridges the gap
% - Compute performance per committees
% - Compute performance per topic

We explore how the \wow{XLT}\ model performs on conflict of different sizes in the test set for EP8 (we observe a similar behaviour on EP7).
We bin the conflict size so that there are at least 100 data points in each bin.
The distribution of conflict size is exponentially decreasing: There are 8462 conflicts of size 1 (i.e., an edit is in conflict with the status quo only), 3063 conflicts of size 2 (i.e., two edits are in conflict, as well as with the status quo), and 140 conflicts of size 7 and more.
We compare the average cross entropy of the \wow{XLT}\ model with that of the random predictor and that of the \wow{}\ model.
In Figure~\ref{lmp:fig:error-analysis}, we see that while the loss generally increases with conflict size for all three models, it increases less rapidly for the \wow{XLT}\ model than for the \wow{}\ model.
This suggests that the explicit, latent, and text features enable the model to exploit the increasing complexity of data points to make more accurate predictions.
We also see that for conflicts of size 4 and higher, the \wow{}\ model performs worse than the random predictor, but the \wow{XLT}\ model is able to outperform it.

\begin{figure}
	\centering
	\includegraphics{lmp-error-analysis}
	\caption{%
		Average cross-entropy loss per conflict size~$\vert \mathcal{C} \vert = K$.
		The loss of the \wow{XLT} model increases less rapidly than the loss of the baselines.
	}
	\label{lmp:fig:error-analysis}
\end{figure}

\subsection{Contribution of Explicit Features}

% - Explore the contribution of different explicit features to the performance of the model
To understand the contribution of the explicit features to the predictive performance, we show in Figure~\ref{lmp:fig:improvement} the decrease in cross-entropy loss of \wow{MEP}\ (all MEP features but the rapporteur feature), \wow{Rapporteur}\ (rapporteur feature only), \wow{Edit}, and \wow{Dossier} over \wow{}.
The dossier features contribute virtually nothing to the predictive performance (the difference is at the fourth decimal point).
Similarly, for EP7, the nationality, political group, and gender features of \wow{MEP} contribute very little.
For EP8, these features improve the performance, but not as much as the edit features.
This suggests that these features have limited influence on the predictions.
Nationalities and political groups have been qualitatively analyzed in the literature in the context of their influence on MEPs' voting behaviour~\cite{hix2002parliamentary,coman2009reassessing,muhlbock2012national,lefkofridi2014multilevel}.
To the best of our knowledge, there is no analysis of their effect on the amending process.
Interestingly, for EP7, combining all features into the \wow{X}\ model leads to a performance boost that is greater than the sum of each individual feature groups.

\begin{figure}
	\centering
	\includegraphics{lmp-improvement}
	\caption{%
		Difference in cross-entropy loss over \wow{} of different models.
		The rapporteur feature and the edit features contribute more to the predictive performance than the MEP and dossier features.
	}
	\label{lmp:fig:improvement}
\end{figure}

\subsection{Interpretation of Explicit Features}

To get insights into the dynamics of the legislative process, we interpret the values of the parameters of \wow{XLT} trained on the full dataset for EP8 (combining training, validation, and test data).
Let~$w_f \in \mathbf{R}$ be the value of the parameter associated with feature~$f$.
The rapporteur feature~$r$ of \wow{Rapp.}\ provides a greater decrease in loss.
This \textit{rapporteur advantage} complements the findings of~\citet{costello2010policy}, conducted by interviewing key informants over EP5 (1999-2004) and EP6 (2004-2009).
They show that the rapporteur, with their particular role, has some influence on the legislative process, albeit constrained.
We note that, according to our model, the rapporteur advantage has slightly increased in EP8 ($w_r=1.19$) compared to EP7 ($w_r=1.12$).

% - Interpret the values that different parameter of the explicit features take
These explicit features enable us to explain what contributes to the success of an edit.
We report here (and in subsequent sections) the results for EP8 only.
All other things being equal, a female ($w_{\text{fem}}=-0.02 > -0.04 = w_{\text{mal}}$) MEP from Latvia and whose party belongs to the group of the European People's Party (center-right) has the highest chance to see her edit accepted.
This edit has even higher chances if it inserts ($w_{\text{ins}}=-0.03 > w_{\text{del}}=-0.13 > w_{\text{rep}}=-0.22$) a short portion of text (the feature associated with both insertion and deletion length is negative) in a part of the law that is not its body or its preamble ($w_{\text{art}}$, $w_{\text{rec}}$  and~$w_{\text{para}}$ have the lowest value among the seven article types).
Adding a justification also increases the probability of an edit being accepted ($w_{\text{jus}}=0.08$), as well as edits from the opinion committee (referred to as the ``outsider committee'' feature in Table~\ref{lmp:tab:features},~$w_{\text{out}} =  0.16$).

For the dossier features, our model learns that it is harder to make edits on reports, as compared to opinions ($w_{\text{rep}}=0.33 > -0.26 = w_{\text{opi}}$).
As explained in Section~\ref{lmp:sec:dataset}, reports are voted by the whole Parliament.
Therefore, they have a greater influence on the final law, and we expect that MEPs make it more difficult for competing edits to be accepted in reports.
Finally, our model also learns that it is harder to make edits for decisions and directives, as compared to regulations ($w_{\text{dec}}=0.25 > w_{\text{dir}} = 0.12 > w_{\text{reg}} =0.10$).

\paragraph{Controversy of Dossiers}

Table~\ref{lmp:tab:inertia_params} provides a list of the ten dossiers in EP8 with the highest inertia parameter~$d_i$ and the ten dossiers with the lowest~$ d_i $.
Overall, the values of $d_i$ correlate well with the number of nodes, the number of cliques, the average size of cliques, and the edit acceptance rate.
These four metrics are a good proxy to the level of activity by MEPs in the amending process of a given dossier.
Higher activity, possibly due to higher controversy, leads to higher value of~$d_i$.
We note, however, that some of the top-10 dossiers have a small number of edits.
This shows that the inertia parameters capture more information than simply some of these descriptive statistics.

The top-ten dossiers include laws with high stakes about financial markets, the environment, vast investment programmes, and assistance to member states:
The ``Screening of foreign direct investments'' sets a framework to better equip the EU for investments from non-EU countries.
It has crucial implications for companies, workers, governments, and citizens.
The ``European Supervisory Authorities on financial markets'' sets strict regulations for the financial markets.
``InvestEU'' and the ``Horizon Programme'' are vast investment programmes for innovation and research.
The ``Cost-effective emission reductions and low-carbon investments'' is one of the implementations of the Paris Climate Agreement.
Finally, The infamous ``Copyright in the Digital Single Market'', considered to be a threat to freedom of expression on the Web by its opponents, sparked public protests in several cities.
The reporting committee publicized that ``MEPs have rarely or never been subject to a similar degree of lobbying before''~\citep{europarl2019questions}.

\begin{sidewaystable}
	\centering
	\caption{Top-10 and bottom-10 inertia parameters~$d_i$ for dossiers in EP8.}
	\label{lmp:tab:inertia_params}
	\begin{tabular}{rlllrrrr}
		\toprule
		$d_i$  & Type & Comm. & Title                                                         & \# edits & \# conf. & avg.\ cf.\ sz. & \% acc. \\
		\midrule

		2.018  & Rep. & INTA  & Screening of foreign direct investments                       & 1040     & 272      & 3.1            & 2.6     \\
		1.958  & Opi. & ITRE  & Cost-effective emission reductions and low-carbon investments & 1756     & 385      & 4.2            & 5.1     \\
		1.879  & Opi. & PETI  & Discontinuing seasonal changes of time                        & 81       & 25       & 2.9            & 6.2     \\
		1.619  & Rep. & ENVI  & Health technology assessment and amending                     & 133      & 14       & 2.0            & 4.5     \\
		1.512  & Rep. & ECON  & European Supervisory Authorities on financial markets         & 48       & 12       & 2.2            & 10.4    \\
		1.447  & Rep. & ECON  & InvestEU Programme                                            & 1194     & 297      & 2.9            & 27.0    \\
		1.393  & Rep. & ITRE  & Horizon Europe                                                & 2013     & 467      & 3.0            & 9.8     \\
		1.386  & Rep. & INTA  & Macro-financial assistance to the Republic of Moldova         & 36       & 8        & 2.4            & 13.9    \\
		1.286  & Rep. & AFET  & Instrument for Pre-Accession Assistance                       & 732      & 239      & 2.5            & 20.6    \\
		1.282  & Rep. & JURI  & Copyright in the Digital Single Market                        & 2657     & 577      & 4.3            & 2.6     \\

		\midrule

		-1.651 & Opi. & REGI  & Common agricultural policy                                    & 105      & 4        & 2.0            & 82.9    \\
		-1.655 & Opi. & DEVE  & Promotion of the use of energy from renewable sources         & 62       & 3        & 2.0            & 90.3    \\
		-1.681 & Opi. & AGRI  & Establishing Horizon Europe                                   & 43       & 8        & 2.0            & 65.1    \\
		-1.686 & Opi. & AGRI  & Governance of the Energy Union                                & 150      & 30       & 2.3            & 56.7    \\
		-1.754 & Opi. & JURI  & Insurance against civil liability with motor vehicles         & 29       & 2        & 2.0            & 89.7    \\
		-1.779 & Opi. & BUDG  & Common agricultural policy                                    & 15       & 0        & 0.0            & 100.0   \\
		-1.780 & Opi. & AGRI  & European Regional Development and Cohesion Fund               & 129      & 13       & 2.2            & 58.1    \\
		-1.812 & Opi. & ECON  & Prevention and prosecution of criminal offences               & 81       & 2        & 2.0            & 86.4    \\
		-2.065 & Opi. & DEVE  & Unfair trading practices in in the food industry              & 63       & 6        & 2.0            & 84.1    \\
		-2.284 & Opi. & TRAN  & Protection of the collective interests of consumers           & 121      & 26       & 2.1            & 66.9    \\

		\bottomrule
	\end{tabular}
\end{sidewaystable}

\subsection{Interpretation of Text Features}
\label{lmp:sec:intertext}

In Figure~\ref{lmp:fig:results}, we observe that the text features contribute significantly to improving the performance.
We use the learned parameter vectors~$\boldsymbol{w}_T$ and~$\boldsymbol{w}_{T'}$ of \wow{XLT}\ to identify words and bigrams that have the most predictive power.
First, we rank the words and bigrams of the edit text, according to the dot product of their embeddings with~$\boldsymbol{w}_T$.
The top-$k$ terms (having a positive dot product) contribute the most towards acceptance of the edit, whereas the bottom-$k$ terms (having a negative dot product) contribute most towards rejection of the edit.
The opposite holds for the terms of the title and their dot product with~$\boldsymbol{w}_{T'}$.

We look at the top~$50$ terms for each feature and prediction outcome and find some interesting patterns among these terms, although not all of them are easy to interpret.
Note that we have more than~\numprint{10000} unique terms for the edit text and more than~\numprint{1000} unique terms for the title, hence we consider only the most predictive terms near the ends of the ranking.
% A list of the top-50 terms for each feature and prediction outcome is reported in Appendix~\ref{app:accept}.

%We first examine the words and bigrams in the inserted and deleted text are predictive of acceptance.
%We see the word \textit{consumer} here, which commonly occurs in laws on consumer rights.
%This suggests that deleting provisions of the laws that give rights or benefits to consumers might be getting accepted often (possibly under the influence of lobbies), and indeed we see many such examples in the dataset.
One of the bigrams that, when deleted, is predictive of acceptance is \textit{any other}, which is commonly used to widen the scope of the law (as in ``contractual or any other duty'').
Interestingly, the bigrams \textit{human rights} and \textit{data protection} are also predictive of acceptance when deleted.
The word \textit{should}, which is used to add recommendations, is predictive of acceptance when inserted, while adding \textit{must}, which is used for obligations, is predictive of rejection.
We see that \textit{best} is predictive of acceptance, which is commonly used to make a requirement stronger (as in ``best available scientific evidence'', ``best possible way'').
Adding \textit{positive} and \textit{positive impact} predicts acceptance, whereas adding \textit{negative} predicts rejection.
Adding the word \textit{inserted}, which commonly refers to inserting new articles in existing laws, is predictive of acceptance, whereas \textit{deleted} is predictive of rejection.

Considering the words in the context, we see that \textit{firearms}, \textit{resettlement}, \textit{terrorist} and \textit{fingerprints} are predictive of rejection.
This could be because the laws related to these topics are controversial, hence many edits are rejected due to conflicts.
For the words in the title, we see that \textit{customs}, \textit{community}, \textit{financial}, \textit{fisheries}, and \textit{general budget} are predictive of acceptance, whereas \textit{market}, \textit{framework}, \textit{structural reform}, \textit{emission},  and \textit{greenhouse gas} are predictive of rejection.
This suggests the relative ease or difficulty of editing laws related to these topics, and it correlates well with the values of the difficulty parameters~$d_i$:
The top-50 dossiers with the highest difficulty parameters contain high-controversy dossiers about establishing frameworks for the screening of foreign investments and vast public investment programs (InvestEU and Horizon Europe), as well as regulation of the financial market, copyright in the digital market, and carbon-emission reduction.
The bottom-50 dossiers with the lowest difficulty parameters contain low-controversy dossiers about cohesion within the EU, financial rules, fisheries, and the community code on visas.

\subsection{Interpretation of Latent Features}
% - Interpretation of the latent features
%   - In terms of ideological space for the dossier and the MEPs
%   - Explains why they improve the performance, by capturing different ideologies rather than explicit party assignment

The latent features improve the predictions overall and help capture the complex dynamics of the legislative process.
The best number of latent dimensions is~$L = 20$  for the models including latent features.
In order to interpret the latent features, we gather the latent vectors~$\bm{y}_i$ learned by \wow{XLT}\ into a matrix~$Y = [ \bm{y}_i ]$.
We apply principal component analysis and keep the top-10 and bottom-10 dossiers from each of the first two principal components in EP8.
We use t-SNE~\cite{maaten2008visualizing} to represent these forty dossiers in a two-dimensional space, and we show the projection in Figure~\ref{lmp:fig:tsne}.

We distinguish four clusters.
The cluster at the top-left contains dossiers about fuel quality, renewable energy, trade of animals, and sustainable investments.
It also contains dossiers about electronic communications, the processing of personal data, and sharing public information.
We interpret this cluster as \textit{environment and communications}, and we highlight with green triangles the corresponding dossiers.
The cluster at the top-center contains dossiers about the establishment of defense funds, the prosecution of criminal offenses, and the identification of criminals between member states.
It also contains dossiers about the protection of workers, businesses, refugees, internal markets, and cultural goods.
We interpret this cluster as \textit{defense and protection} (red crosses).
The cluster at the top-right contains dossiers about vast investment and development programmes, finance, and the development of internal markets.
We interpret this cluster as \textit{investment and development} (blue dots).
Finally, the cluster at the bottom-left contains dossiers about economic competitiveness and innovation, as well as frameworks for business development and the funding of start-up companies.
We interpret this cluster as \textit{business and innovation} (orange squares).

\begin{figure}
	\centering
	\includegraphics{lmp-tsne}
	\caption{Visualization with t-SNE of the top-10 and bottom-10 dossiers on the first two principal components in EP8.
		There are four clusters:
		Environment and Communications, Defense and Protection, Investment and Development, and Business and Innovation.
	}
	\label{lmp:fig:tsne}
\end{figure}

\subsection{Solving the Cold-Start Problem}
\label{lmp:sec:cold-start}

% - Predicting an edit for a new dossier
We explore how to solve the cold-start problem by defining a second predictive problem:
Given a dossier~$i$ \textit{for which we have never seen an edit}, and given a conflict~$\mathcal{C} = \{a, b, \ldots \}$, we want to predict which of the edits or the status quo wins.
We order the dossiers by the date a committee received a proposal, and we use the dossiers that contain the first 80\% of the conflicts as a training set.
We use the next 10\% as validation set, and we keep the last 10\% aside as test set.
We ensure that no edits in the training set leak into the validation and test sets.
This scenario is more realistic because we make predictions about new dossiers that the model has never observed before.

We report, in Table~\ref{lmp:tab:newdossier}, the results for \wow{Explicit}, \wow{Text}, and \wow{XT}, together with the baselines.
The latent features cannot be used for this task, as the dossier embeddings~$\vec{y}_i$ are unavailable for new dossiers.
For our models, the difficulty parameter~$d_i$ is set to the average difficulty learned in the training set.
The random predictor, which learns the prior probability of the status quo winning for each conflict size, performs the best out of all the baselines, and it outperforms \wow{Text}.
Our approach outperforms only the random predictor when including explicit features.
This suggests that the dossier features help us make more accurate predictions by learning parameter values for the type of dossier, its legal act, and its committee in charge.
In this case, adding text features further boosts the performance.

The overall performance, however, is mixed:
The improvement of \wow{XT} over the random predictor is rather small.
One possible explanation is that the legislative process might be non-stationary.
Hence, our model overfits on the training set, which is very different from the test set.
The task is also unfair to our model, as in a real setting, predictions would be made for the next dossier only.
In the current setting, we make predictions for all future dossiers.
We keep further investigations of this aspect for future work.

\begin{table}
	\centering
	\caption{Average cross entropy of the baselines and our model on predicting new, unseen dossiers.}
	\label{lmp:tab:newdossier}
	\begin{tabular}{llr}
		\toprule
		Type     & Model          & Avg.\ cross entropy \\
		\midrule
		Baseline & Naive          & 0.947               \\
		         & Random         & \textbf{0.800}      \\
		         & \wow{}         & 0.873               \\
		\midrule
		Ours     & \wow{Explicit} & 0.784               \\
		         & \wow{Text}     & 0.839               \\
		         & \wow{XT}       & \textbf{0.759}      \\
		\bottomrule
	\end{tabular}
\end{table}
