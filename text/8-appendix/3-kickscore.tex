%! TEX root = ../thesis.tex
\section{Supplementary Material for Chapter~\ref{ch:kickscore}}
\label{kks:app:suppmat}

\subsection{Inference Algorithm}
\label{kks:app:inference}

For conciseness, we drop the index $n$ and consider a single observation $(\bm{x}, t^*, y) \in \mathcal{D}$.
Let $\bm{s} \define \bm{s}(t^*)$ be the vector containing the score of all features at the time of the observation.
Instead of optimizing the ``standard'' parameters $\tilde{\mu}_m, \tilde{\sigma}^2_m$, we will optimize the corresponding \emph{natural} parameters $\tilde{\alpha}_m, \tilde{\beta}_m$.
They are related through the following equations.
\begin{align*}
	\tilde{\alpha}_m = \tilde{\mu}_m / \tilde{\sigma}^2_m, \qquad
	\tilde{\beta}_m = 1 / \tilde{\sigma}^2_m
\end{align*}

\paragraph{Expectation Propagation}

Let $q_{-}(\bm{s}) = \DNorm{\bm{\mu}, \bm{\Sigma}}$ be the cavity distribution.
The log-partition function can be rewritten as a one-dimensional integral:
\begin{align*}
	\log Z
	 & \define \log \Exp[q_{-}]{ p(y \mid \bm{x}\Tr \bm{s}) }
	= \log \int_{\bm{s}} p(y \mid \bm{x}\Tr \bm{s}) \DNorm{\bm{s} \mid \bm{\mu}, \bm{\Sigma}} d\bm{s} \\
	 & = \log \int_{u} p(y \mid u) \DNorm{u \mid \mu, \sigma^2} du,
\end{align*}
where $\mu = \bm{x}\Tr \bm{\mu}$ and $\sigma^2 = \bm{x}\Tr \bm{\Sigma} \bm{x}$.
The function \textsc{Derivatives} computes the first and second derivatives with respect to this mean, i.e.,
\begin{align*}
	\delta_1 = \frac{\partial}{\partial \mu} \log Z, \qquad
	\delta_2 = \frac{\partial^2}{\partial \mu^2} \log Z.
\end{align*}
Given these quantities, the function \textsc{UpdateParams} updates the pseudo-observations' parameters for each $m \in \mathcal{X}$:
\begin{align*}
	\tilde{\alpha}_m & \gets (1 - \lambda) \tilde{\alpha}_m
	+ \lambda \left[ \frac{x_m \delta_1 - \mu_m x_m^2 \delta_2}{1 + \Sigma_{mm} x_m^2 \delta_2} \right], \\
	\tilde{\beta}_m  & \gets (1 - \lambda) \tilde{\beta}_m
	+ \lambda \left[ \frac{-x_m^2 \delta_2}{1 + \Sigma_{mm} x_m^2 \delta_2} \right],
\end{align*}
where $\lambda \in (0, 1]$ is a learning rate.
A formal derivation of these update equations can be found in \citet{seeger2007bayesian, rasmussen2006gaussian, minka2001family}.

\paragraph{Reverse KL Divergence}

Let $q(\bm{s}) = \DNorm{\bm{\mu}, \bm{\Sigma}}$ be the current posterior.
Similarly to the log-partition function, the expected log-likelihood can be rewritten as a one-dimensional integral:
\begin{align*}
	L
	 & \define \Exp[q]{ \log p(y \mid \bm{x}\Tr \bm{s}) }
	= \int_{\bm{s}} \log p(y \mid \bm{x}\Tr \bm{s}) \DNorm{\bm{s} \mid \bm{\mu}, \bm{\Sigma}} d\bm{s} \\
	 & = \int_{u} \log p(y \mid u) \DNorm{u \mid \mu, \sigma^2} du,
\end{align*}
where $\mu = \bm{x}\Tr \bm{\mu}$ and $\sigma^2 = \bm{x}\Tr \bm{\Sigma} \bm{x}$.
The function \textsc{Derivatives} computes the first and second derivatives with respect to this mean, i.e.,
\begin{align*}
	\delta_1 = \frac{\partial}{\partial \mu} L, \qquad
	\delta_2 = \frac{\partial^2}{\partial \mu^2} L.
\end{align*}
Given these quantities, the function \textsc{UpdateParams} updates the pseudo-observations' parameters for each $m \in \mathcal{X}$:
\begin{align*}
	\tilde{\alpha}_m & \gets (1 - \lambda) \tilde{\alpha}_m
	+ \lambda \left[ x_m \delta_1 - \mu_m x_m^2 \delta_2 \right], \\
	\tilde{\beta}_m  & \gets (1 - \lambda) \tilde{\beta}_m
	+ \lambda \left[ -x_m^2 \delta_2 \right],
\end{align*}
where $\lambda \in (0, 1]$ is the learning rate.
A formal derivation of these update equations can be found in \citet{khan2017conjugate}.

\subsection{Experimental Evaluation}
\label{kks:app:eval}

The code used to produce the experiments presented in this paper is publicly available online.
It consists of two software libraries.
\begin{itemize}
	\item A library written in the Python programming language, available at \url{https://github.com/lucasmaystre/kickscore}.
	      This libary provides a reference implementation of Algorithm~\ref{kks:alg:inference} with a user-friendly API.

	\item A library written in the Go programming language, available at \url{https://github.com/lucasmaystre/gokick}.
	      This library provides a multithreaded implementation of Algorithm~\ref{kks:alg:inference}, focused on computational performance.
\end{itemize}
Additionally, the scripts and computational notebooks used to produce the experiments and figures presented in this paper are available at \url{https://github.com/lucasmaystre/kickscore-kdd19}.

\subsubsection{Hyperparameters}

Generally speaking, we choose hyperparameters based on a search over \num{1000} configurations sampled randomly in a range of sensible values (we always make sure that the best hyperparameters are not too close to the ranges' boundaries).
In the case of our models, we choose the configuration that maximizes the log-marginal likelihood of the training data.
In the case of TrueSkill and Elo, we choose the configuration that minimizes the leave-one-out log loss on the training data.
A list of all the hyperparameters is provided in Table~\ref{kks:tab:hpdefs}, and a formal definition of the covariance functions we use is given in Table~\ref{kks:tab:covfuncs}.

\begin{table}
	\centering
	\caption{
		Hyperparameters and their description.}
	\label{kks:tab:hpdefs}
	\renewcommand{\arraystretch}{1.2}
	\begin{tabular}{c l}
		\toprule
		Symbol                  & Description                             \\
		\midrule
		$\lambda$               & Learning rate                           \\
		$\alpha$                & Draw margin                             \\
		$\sigma^2_n$            & Observation noise (Gaussian likelihood) \\
		$\sigma_{\text{cst}}^2$ & Variance (constant covariance)          \\
		$\sigma_{\text{lin}}^2$ & Variance (linear covariance)            \\
		$\sigma_{\text{W}}^2$   & Variance (Wiener covariance)            \\
		$\nu$                   & Smoothness (Matérn covariance)          \\
		$\sigma_{\text{dyn}}^2$ & Variance (Matérn covariance)            \\
		$\ell$                  & Timescale, in years (Matérn covariance) \\
		\bottomrule
	\end{tabular}
\end{table}

\begin{table}
	\centering
	\caption{
		Covariance functions.}
	\label{kks:tab:covfuncs}
	\renewcommand{\arraystretch}{1.2}
	\begin{tabular}{l l}
		\toprule
		Name                & $k(t, t')$                                                                                     \\
		\midrule
		Constant            & $\sigma_{\text{cst}}^2$                                                                        \\
		Linear              & $\sigma_{\text{lin}}^2 t t'$                                                                   \\
		Wiener              & $\sigma_{\text{W}}^2 \min \{ t, t' \}$                                                         \\
		Matérn, $\nu = 1/2$ & $\sigma_{\text{dyn}}^2 \exp (-\Abs{t - t'} / \ell)$                                            \\
		Matérn, $\nu = 3/2$ & $\sigma_{\text{dyn}}^2 (1 +\sqrt{3} \Abs{t - t'} / \ell) \exp (-\sqrt{3} \Abs{t - t'} / \ell)$ \\
		\bottomrule
	\end{tabular}
\end{table}

% \begin{table}
% 	\centering
% 	\caption{
% 		Hyperparameter values used for the models of Section~\ref{kks:sec:eval}.}
% 	\label{kks:tab:hpvals}
% 	\sisetup{table-format=1.3}
% 	\begin{tabular}{lll SSS[table-format=3.3]SSScS[table-format=2.3]S[table-format=3.3]}
% 		\toprule
% 		Dataset         & Model                     & Likelihood                & {$\lambda$}             & {$\alpha$} & {$\sigma^2_n$}
% 		                & {$\sigma_{\text{cst}}^2$} & {$\sigma_{\text{lin}}^2$} & {$\sigma_{\text{W}}^2$}
% 		                & {$\nu$}                   & {$\sigma_{\text{dyn}}^2$} & {$\ell$}                                                                                                \\
% 		\midrule        % Model                   % likelihood  %   lr  % alpha %   s_n   % s_cst % s_lin %  s_W  %  nu  %  s_dyn %   ell
% 		ATP tennis      & Constant                  & Probit                    & 1.000                   & \Emd       & \Emd           & 0.817 & \Emd  & \Emd  & \Emd & \Emd   & \Emd    \\
% 		                & Elo                       & Logit                     & 0.262                   & \Emd       & \Emd           & \Emd  & \Emd  & \Emd  & \Emd & \Emd   & \Emd    \\
% 		                & TrueSkill                 & Probit                    & \Emd                    & \Emd       & \Emd           & 0.137 & \Emd  & 0.007 & \Emd & \Emd   & \Emd    \\
% 		                & Ours                      & Probit                    & 1.000                   & \Emd       & \Emd           & 0.366 & 0.001 & 0.147 & \Emd & \Emd   & \Emd    \\
% 		                & Figure~\ref{fig:scores}   & Probit                    & 1.000                   & \Emd       & \Emd           & 0.034 & \Emd  & \Emd  & 3/2  & 0.912  & 7.469   \\
% 		\midrule
% 		NBA Basketball  & Constant                  & Probit                    & 1.000                   & \Emd       & \Emd           & 0.060 & \Emd  & \Emd  & \Emd & \Emd   & \Emd    \\
% 		                & Elo                       & Logit                     & 0.095                   & \Emd       & \Emd           & \Emd  & \Emd  & \Emd  & \Emd & \Emd   & \Emd    \\
% 		                & TrueSkill                 & Probit                    & \Emd                    & \Emd       & \Emd           & 0.128 & \Emd  & 0.001 & \Emd & \Emd   & \Emd    \\
% 		                & Ours                      & Probit                    & 1.000                   & \Emd       & \Emd           & 0.003 & \Emd  & \Emd  & 1/2  & 0.152  & 3.324   \\
% 		                & Figure~\ref{fig:scores}   & Probit                    & 1.000                   & \Emd       & \Emd           & 0.003 & \Emd  & \Emd  & 3/2  & 0.138  & 1.753   \\
% 		                & Table~\ref{tab:lklperf}   & Logit                     & 1.000                   & \Emd       & \Emd           & 0.001 & \Emd  & \Emd  & 1/2  & 0.417  & 3.429   \\
% 		                & Table~\ref{tab:lklperf}   & Gaussian                  & 1.000                   & \Emd       & 143.451        & 0.059 & \Emd  & \Emd  & 1/2  & 17.667 & 3.310   \\
% 		                & Table~\ref{tab:lklperf}   & Poisson-exp               & 0.800                   & \Emd       & \Emd           & 5.470 & \Emd  & \Emd  & 1/2  & 0.003  & 2.378   \\
% 		\midrule
% 		World Football  & Constant                  & Probit                    & 1.000                   & 0.372      & \Emd           & 0.933 & \Emd  & \Emd  & \Emd & \Emd   & \Emd    \\
% 		                & Elo                       & Logit                     & 0.196                   & 0.578      & \Emd           & \Emd  & \Emd  & \Emd  & \Emd & \Emd   & \Emd    \\
% 		                & TrueSkill                 & Probit                    & \Emd                    & 0.381      & \Emd           & 1.420 & \Emd  & 0.001 & \Emd & \Emd   & \Emd    \\
% 		                & Ours                      & Probit                    & 1.000                   & 0.386      & \Emd           & 0.750 & \Emd  & \Emd  & 1/2  & 0.248  & 69.985  \\
% 		                & Table~\ref{tab:lklperf}   & Logit                     & 1.000                   & 0.646      & \Emd           & 2.001 & \Emd  & \Emd  & 1/2  & 0.761  & 71.693  \\
% 		                & Table~\ref{tab:lklperf}   & Gaussian                  & 1.000                   & \Emd       & 3.003          & 4.062 & \Emd  & \Emd  & 1/2  & 2.922  & 175.025 \\
% 		                & Table~\ref{tab:lklperf}   & Poisson-exp               & 0.800                   & \Emd       & \Emd           & 0.300 & \Emd  & \Emd  & 1/2  & 0.210  & 83.610  \\
% 		                & Table~\ref{tab:homeadv}   & Probit                    & 1.000                   & 0.407      & \Emd           & 0.895 & \Emd  & \Emd  & 1/2  & 0.220  & 44.472  \\
% 		\midrule
% 		ChessBase small & Constant                  & Probit                    & 1.000                   & 0.554      & \Emd           & 0.364 & \Emd  & \Emd  & \Emd & \Emd   & \Emd    \\
% 		                & Elo                       & Logit                     & 0.157                   & 0.856      & \Emd           & \Emd  & \Emd  & \Emd  & \Emd & \Emd   & \Emd    \\
% 		                & TrueSkill                 & Probit                    & \Emd                    & 0.555      & \Emd           & 0.240 & \Emd  & 0.001 & \Emd & \Emd   & \Emd    \\
% 		                & Ours                      & Probit                    & 1.000                   & 0.558      & \Emd           & 0.307 & \Emd  & 0.010 & \Emd & \Emd   & \Emd    \\
% 		                & Table~\ref{tab:homeadv}   & Probit                    & 1.000                   & 0.568      & \Emd           & 0.188 & \Emd  & \Emd  & 1/2  & 0.188  & 35.132  \\
% 		\bottomrule
% 	\end{tabular}
% \end{table}

\subsubsection{Capturing Intransitivity}
\label{kks:app:intrans}

We closely follow the experimental procedure of \citet{chen2016modeling} for the experiment of Section~\ref{kks:sec:eval-intrans}.
In particular, we randomly partition each dataset into three splits: a training set (\num{50}\% of the data), a validation set (\num{20}\%), and a test set (\num{30}\%).
We train the model on the training set, choose hyperparameters based on the log loss measured on the validation set, and finally report the average log loss on the test set.

For the Blade-Chest model, we were not able to reproduce the exact results presented in \citet{chen2016modeling} using the open-source implementation available at \url{https://github.com/csinpi/blade_chest}.
Instead, we just report the best values in Figures 3 and 4 of the paper (\emph{blade-chest inner} model, with $d = 50$).

For the Bradley--Terry model, we use the Python library \emph{choix} and its \texttt{opt\_pairwise} function.
The only hyperparameter to set is the regularization strength $\alpha$.

For our model, since there are no timestamps in the StarCraft datasets, we simply use constant covariance functions.
The two hyperparameters are $\sigma^2_{\text{cst}}$ and $\sigma^2_{\times}$, the variance of the player features and of the interaction features, respectively.
The hyperparameter values that we used are given in Table~\ref{kks:tab:hpintrans}.

\begin{table}
	\centering
	\caption{
		Hyperparameter values for the experiment of Section~\ref{kks:sec:eval-intrans}.}
	\label{kks:tab:hpintrans}
	\sisetup{table-format=1.3}
	\begin{tabular}{l S SS}
		\toprule
		               & \text{Bradley--Terry} & \multicolumn{2}{c}{Ours}                          \\
		\cmidrule(r){2-2}       \cmidrule{3-4}
		Dataset        & {$\alpha$}            & {$\sigma^2_{\text{cst}}$} & {$\sigma^2_{\times}$} \\
		\midrule
		StarCraft WoL  & 0.077                 & 4.821                     & 3.734                 \\
		StarCraft HotS & 0.129                 & 4.996                     & 4.342                 \\
		\bottomrule
	\end{tabular}
\end{table}
