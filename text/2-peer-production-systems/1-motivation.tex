%! TEX root = ../thesis.tex
\section{Motivation}
\label{pps:sec:intro}

Over the last two decades, the number and scale of online peer-production systems has become truly massive, driven by better information networks and advances in collaborative software.
At the time of writing, \num{128643} editors contribute regularly to \num{5}+ million articles of the English Wikipedia \citep{wikipedia2017wikipedians} and over \num{15600} developers have authored code for the Linux kernel \citep{corbet2017linux}.
On GitHub, \num{24} million users collaborate on \num{25.3} million active software repositories \citep{github2017octoverse}.

In order to ensure that such projects advance towards their goals, it is necessary to identify whether edits made by users are beneficial.
As the number of users and components of the project grows, this task becomes increasingly challenging.
In response, two types of solutions are proposed.
On the one hand, some advocate the use of \emph{user reputation systems} \citep{resnick2000reputation, adler2007content}.
These systems are general, their predictions are easy to interpret and can be made resistant to manipulations \citep{dealfaro2013content}.
On the other hand, a number of highly specialized methods are proposed to automatically predict the quality of edits in particular peer-production systems \citep{druck2008learning, wikimedia2015artificial}.
%For example, Wikipedia has developed a service which predicts whether an edit will be damaging by analyzing \num{80}+ content-based and system-based features of the user, the article and the edit itself, such as:
%the number of bad words introduced by the edit, the time since the user's registration and the length of the article \citep{wikimedia2015artificial}.
These methods can attain excellent predictive performance \citep{heindorf2016vandalism} and usually significantly outperform predictors that are based on user reputation alone \citep{druck2008learning}, but they are tailored to a particular peer-production system, use domain-specific features and rely on models that are difficult to interpret.

In this work, we set out to explore another point in the solution space.
We aim to keep the generality and simplicity of user reputation systems, while reaching the predictive accuracy of highly specialized methods.
We ask the question:
Can one predict the outcome of contributions simply by observing \emph{who edits what} and whether the edits eventually survive?
We address this question by proposing a novel statistical model of edit outcomes.
We formalize the notion of collaborative project as follows.
$N$ users can propose edits on $M$ distinct items (components of the project, such as articles on Wikipedia or a software's modules), and we assume that there is a process for validating edits (either immediately or over time).
We observe triplets $(u, i, q)$ that describe a user $u \in \{1, \ldots, N\}$ editing an item $i \in \{1, \ldots, M\}$ and leading to outcome $q \in \{0, 1\}$;
the outcome $q = 0$ represents a rejected edit, whereas $q = 1$ represents an accepted, beneficial edit.
Given a dataset of such observations, we seek to learn a model of the probability $p_{ui}$ that an edit made by user $u$ on item $i$ is accepted.
This model can then be used to help moderators and project maintainers prioritize their efforts once new edits appear:
For example, edits that are unlikely to survive could be sent out for review immediately.

Our approach borrows from probabilistic models of pairwise comparisons \citep{zermelo1928berechnung, rasch1960probabilistic}.
These models learn a real-valued score for each object (user or item) such that the difference between two objects' scores is predictive of comparison outcomes.
We take a similar perspective and view each edit in a collaborative project as a game between the user who tries to effect change and the item that resists change\footnote{Obviously, items do not really ``resist'' by themselves. Instead, this notion should be taken as a proxy for the combined action of other users (e.g., project maintainers) who can accept or reject an edit depending, among others, on standards of quality.}.
Similarly to pairwise-comparison models, our approach learns a real-valued score for each user and each item.
In addition, it also learns latent features of users and items that capture interaction effects.

In contrast to quality-prediction methods specialized on a particular peer-production system, our approach is general and can be applied to any system in which users contribute by editing discrete items.
It does not use any explicit content-based features: instead, it simply learns by observing triplets $\{ (u, i, q) \}$.
Furthermore, the resulting model parameters can be interpreted easily.
They enable a principled way of
\begin{enuminline}
	\item ranking users by the quality of their contributions,
	\item ranking items by the difficulty of editing them and
	\item understanding the main dimensions of the interaction between users and items.
\end{enuminline}

We apply our approach on two different peer-production systems.
We start with Wikipedia and consider its Turkish and French editions.
Evaluating the accuracy of predictions on an independent set of edits, we find that our model approaches the performance of the state of the art.
More interestingly, the model parameters reveal important facets of the system.
For example, we characterize articles that are easy or difficult to edit, respectively, and we identify clusters of articles that share common editing patterns.
Next, we turn our attention to the Linux kernel.
In this project, contributors are typically highly skilled professionals, and the edits that they make affect \num{394} different subsystems (kernel components).
In this instance, our model's predictions are \emph{more accurate} than a random forest classifier trained on domain-specific features.
In addition, we give an interesting qualitative description of subsystems based on their difficulty score.

In short, our paper
\begin{enuminline}
	\item gives evidence that observing \emph{who edits what} can yield valuable insights into peer-production systems and
	\item proposes a statistically grounded and computationally inexpensive method to do so.
\end{enuminline}
The analysis of two peer-production systems with very distinct characteristics demonstrates the generality of the approach.

\paragraph{Organization of the Paper}
We start by reviewing related literature in Section~\ref{pps:sec:relwork}.
In Section~\ref{pps:sec:models}, we describe our statistical model of edit outcomes and briefly discuss how to efficiently learn a model from data.
In Sections~\ref{pps:sec:wikipedia} and~\ref{pps:sec:linux}, we investigate our approach in the context of Wikipedia and of the Linux kernel, respectively.
Finally, we conclude in Section~\ref{pps:sec:conclusion}.
